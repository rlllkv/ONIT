import requests
import json
import logging
import time
from datetime import datetime
from sqlalchemy import create_engine, text

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
DB_USER = "trudvsem_parser"
DB_PASS = "123"
DB_HOST = "localhost"
DB_PORT = "5432"
DB_NAME = "trudvsem_analytics"

API_URL = "http://opendata.trudvsem.ru/api/v1/vacancies"

# –¶–µ–ª—å: —Å–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π —Ö–æ—Ç–∏–º
TARGET_TOTAL_RECORDS = 5000 
BATCH_SIZE = 20 

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'application/json',
    'Connection': 'keep-alive'
}

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger(__name__)

DATABASE_URI = f'postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}'

# --- –§–£–ù–ö–¶–ò–ò ---

def safe_float(value):
    if value is None: return None
    if isinstance(value, (float, int)): return float(value)
    if isinstance(value, str):
        val = value.strip()
        if not val: return None
        try: return float(val)
        except ValueError: return None
    return None

def get_data_with_retry(offset, retries=5):
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ï—Å–ª–∏ 500 –æ—à–∏–±–∫–∞ ‚Äî –∂–¥–µ—Ç –∏ –ø—Ä–æ–±—É–µ—Ç —Å–Ω–æ–≤–∞.
    """
    params = {'limit': BATCH_SIZE, 'offset': offset}
    for attempt in range(1, retries + 1):
        try:
            response = requests.get(API_URL, params=params, headers=HEADERS, timeout=60)
            
            # –ï—Å–ª–∏ 500-–µ –æ—à–∏–±–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞ ‚Äî –∫–∏–¥–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –≤—Ä—É—á–Ω—É—é, —á—Ç–æ–±—ã –ø–æ–ø–∞—Å—Ç—å –≤ except
            if 500 <= response.status_code < 600:
                response.raise_for_status()
                
            response.raise_for_status() # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–æ–¥–æ–≤ (404, 403)
            response.encoding = 'utf-8'
            return response.json()
            
        except requests.exceptions.RequestException as e:
            wait_time = attempt * 2 # –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: 2, 4, 6, 8, 10 —Å–µ–∫
            logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{retries} –Ω–µ—É–¥–∞—á–Ω–∞ (HTTP Error/Timeout). –ñ–¥–µ–º {wait_time}—Å–µ–∫... –û—à–∏–±–∫–∞: {e}")
            time.sleep(wait_time)
            
    logger.error("‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –±–∞—Ç—á.")
    return None

def init_db(engine):
    ddl_raw = """
    CREATE TABLE IF NOT EXISTS raw_data_log (
        id SERIAL PRIMARY KEY,
        loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        offset_val INTEGER,
        raw_json JSONB
    );
    """
    ddl_mart = """
    CREATE TABLE IF NOT EXISTS vacancies_mart (
        vacancy_uuid TEXT PRIMARY KEY,
        job_title TEXT,
        salary_min INTEGER,
        salary_max INTEGER,
        region_name TEXT,
        specialisation TEXT,
        latitude FLOAT,
        longitude FLOAT,
        date_published DATE,
        url TEXT,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    """
    with engine.begin() as conn:
        conn.execute(text(ddl_raw))
        conn.execute(text(ddl_mart))

# --- MAIN ---

def run_etl():
    try:
        engine = create_engine(DATABASE_URI, pool_pre_ping=True)
        init_db(engine)
        logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î —É—Å–ø–µ—à–Ω–æ. –¢–∞–±–ª–∏—Ü—ã –≥–æ—Ç–æ–≤—ã.")
    except Exception as e:
        logger.critical(f"üî• –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –ë–î: {e}")
        return

    # –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ SQL-–∑–∞–ø—Ä–æ—Å—ã
    sql_raw = text("INSERT INTO raw_data_log (loaded_at, offset_val, raw_json) VALUES (:loaded_at, :offset, :raw_json)")
    sql_mart = text("""
        INSERT INTO vacancies_mart (
            vacancy_uuid, job_title, salary_min, salary_max, region_name, 
            specialisation, latitude, longitude, date_published, url, updated_at
        ) VALUES (
            :uuid, :title, :s_min, :s_max, :reg, 
            :spec, :lat, :lon, :d_pub, :url, :now
        )
        ON CONFLICT (vacancy_uuid) DO UPDATE SET
            job_title = EXCLUDED.job_title,
            salary_min = EXCLUDED.salary_min,
            salary_max = EXCLUDED.salary_max,
            updated_at = EXCLUDED.updated_at;
    """)

    current_offset = 0
    total_saved = 0

    logger.info(f"üöÄ –°—Ç–∞—Ä—Ç –≤—ã–≥—Ä—É–∑–∫–∏. –¶–µ–ª—å: {TARGET_TOTAL_RECORDS} –∑–∞–ø–∏—Å–µ–π.")
    
    try:
        while total_saved < TARGET_TOTAL_RECORDS:
            
            # 1. EXTRACT —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
            data = get_data_with_retry(current_offset)
            
            if data is None:
                logger.error("API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å.")
                break

            vacancies = data.get("results", {}).get("vacancies", [])
            if not vacancies:
                logger.info("üèÅ API –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫. –í–∞–∫–∞–Ω—Å–∏–∏ –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å.")
                break

            # 2. LOAD & TRANSFORM (–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏—è)
            with engine.begin() as conn:
                # –ê. –õ–æ–≥
                conn.execute(sql_raw, {
                    "loaded_at": datetime.now(),
                    "offset": current_offset,
                    "raw_json": json.dumps(data, ensure_ascii=False)
                })

                # –ë. –í–∏—Ç—Ä–∏–Ω–∞
                batch_count = 0
                for item in vacancies:
                    vac = item.get("vacancy", {})
                    if not vac.get("id"): continue
                    
                    try:
                        # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                        lat, lng = None, None
                        addrs = vac.get("addresses", {}).get("address", [])
                        if isinstance(addrs, list) and addrs and isinstance(addrs[0], dict):
                            lat = safe_float(addrs[0].get("lat"))
                            lng = safe_float(addrs[0].get("lng"))

                        dto = {
                            "uuid": vac.get("id"),
                            "title": vac.get("job-name"),
                            "s_min": safe_float(vac.get("salary_min")),
                            "s_max": safe_float(vac.get("salary_max")),
                            "reg": vac.get("region", {}).get("name") if isinstance(vac.get("region"), dict) else None,
                            "spec": vac.get("category", {}).get("specialisation") if isinstance(vac.get("category"), dict) else None,
                            "lat": lat, "lon": lng,
                            "d_pub": vac.get("creation-date"),
                            "url": vac.get("vac_url"),
                            "now": datetime.now()
                        }
                        conn.execute(sql_mart, dto)
                        batch_count += 1
                    except Exception:
                        continue # –û—à–∏–±–∫–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –Ω–µ –ª–æ–º–∞–µ—Ç –±–∞—Ç—á
                
                total_saved += batch_count
                # === –ö–û–ù–ï–¶ –¢–†–ê–ù–ó–ê–ö–¶–ò–ò (–¢–£–¢ –ü–†–û–ò–°–•–û–î–ò–¢ COMMIT) ===
            
            logger.info(f"üíæ –ë–∞—Ç—á —Å–æ—Ö—Ä–∞–Ω–µ–Ω (offset={current_offset}). –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {total_saved}")
            
            current_offset += BATCH_SIZE
            time.sleep(0.3)

    except KeyboardInterrupt:
        logger.warning("\nüõë –°–∫—Ä–∏–ø—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (Ctrl+C).")
        logger.warning(f"–î–∞–Ω–Ω—ã–µ, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –¥–æ —ç—Ç–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞ ({total_saved} —à—Ç), –ª–µ–∂–∞—Ç –≤ –±–∞–∑–µ.")
    
    logger.info("–†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

if __name__ == "__main__":
    run_etl()